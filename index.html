<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows</title>

    <script src="template.v2.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.16.0/d3.min.js"></script>

    <script>
      window.MathJax = {
        loader: {load: ['[tex]/boldsymbol']},
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['\\[','\\]'], ['$$','$$']],
          packages: {'[+]': ['ams', 'textmacros', 'boldsymbol']},
          macros: {
            vx: "{\\boldsymbol{x}}",
            vphi: "{\\boldsymbol{\\phi}}",
            vF: "{\\boldsymbol{F}}",
            vG: "{\\boldsymbol{G}}",
            vI: "{\\boldsymbol{I}}",
            vz: "{\\boldsymbol{z}}",
            vn: "{\\boldsymbol{n}}",
            vu: "{\\boldsymbol{u}}",
            vf: "{\\boldsymbol{f}}",
            sg: "\\operatorname{sg}",
            vvG: "{\\boldsymbol{v}_\\text{G}}",
            vvN: "{\\boldsymbol{v}_\\text{N}}",
            E: "\\mathbb{E}",
            dtau: "\\,d\\tau"
          }
        },
        svg: { fontCache: 'global' }
      };
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

    <link href="https://db.onlinewebfonts.com/c/860c3ec7bbc5da3e97233ccecafe522e?family=Bitstream+Charter" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@400;500&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="style.css">

    <style>
        body {
            font-family: 'Bitstream Charter', 'Charter', serif;
            font-size: 18px;
            line-height: 1.6;
        }
        /* Layout corrections */
        .display-table th, .display-table td { text-align: center; }
        .display-table th:first-child, .display-table td:first-child { text-align: left; }
        .mjx-chtml { font-family: 'Bitstream Charter', serif !important; }

        /* Style for equation labels */
        .eq-caption {
            font-family: 'Bitstream Charter', serif;
            font-style: italic;
            color: #666;
            font-size: 0.85rem;
            text-align: right; /* Or center, depending on preference */
            margin-top: -0.5rem;
            margin-bottom: 1.5rem;
            margin-right: 1rem;
        }

        /* Wrapfig Utility */
        .wrap-container {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            margin: 2rem 0;
        }
        .wrap-text { flex: 1.2; text-align: justify; }
        .wrap-fig { flex: 0.8; margin: 0; }
        @media (max-width: 800px) { .wrap-container { flex-direction: column; } }

        section { margin-top: 3rem; margin-bottom: 3rem; }
        p { text-align: justify; margin-bottom: 1.5rem; }
    </style>
</head>

<body>

    <div class="header-container">
        <div class="header-content">
            <h1>SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows</h1>
            <p>We propose a simple end-to-end framework to train VAEs and NFs from scratch, outperforming the prior NF-based model STARFlow.</p>
            <div class="button-container">
                <a href="http://arxiv.org/abs/2511.19428" class="button">Paper</a>
                <a href="#" class="button">Code (under review)</a>
                <a href="#" class="button">Models (under review)</a>
            </div>
        </div>
    </div>

    <d-article>
        <p class="affiliations">
            <a href="https://www.linkedin.com/in/qinyu-zhao/" target="_blank">Qinyu&nbsp;Zhao</a><sup>1,2</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com/citations?user=IS0-qKcAAAAJ&hl=zh-CN" target="_blank">Guangting&nbsp;Zheng</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com/citations?user=qT5psCEAAAAJ&hl=zh-CN" target="_blank">Tao&nbsp;Yang</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com/citations?user=DS4fvJ8AAAAJ&hl=zh-CN" target="_blank">Rui&nbsp;Zhu</a><sup>2&dagger;</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com/citations?user=GQzvqS4AAAAJ&hl=en" target="_blank">Xingjian&nbsp;Leng</a><sup>1</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://users.cecs.anu.edu.au/~sgould/" target="_blank">Stephen&nbsp;Gould</a><sup>1</sup>&ensp; <b>&middot;</b> &ensp;
            <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang&nbsp;Zheng</a><sup>1&Dagger;</sup>&ensp;
          </p>
          
          <p class="affiliations">
            <sup>1</sup> Australian National University &emsp; <sup>2</sup> ByteDance Seed &emsp; <br>
          </p>
          <p class="affiliations">
            <sup>&Dagger;</sup> Corresponding Author &emsp; <sup>&dagger;</sup>Project Lead &emsp; <br>
          </p>

        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#overview">Overview</a></div>
                <div><a href="#insights">Key Insights</a></div>
                <div><a href="#method">SimFlow</a></div>
                <div><a href="#results">Experiments</a></div>
                <div><a href="#conclusion">Conclusion</a></div>
            </nav>
        </d-contents>

        <section id="overview">
            <h2>Overview</h2>
            <p>
                Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. 
                <ul style="list-style-type: disc;">
                <li><strong>First</strong>, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. </li>
                <li><strong>Second</strong>, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. </li>
                </ul>
            </p>
        </section>

        <section id="method">
            <h2>Method: SimFlow</h2>
            <p>We propose SimFlow to improve the performance of NFs. Our key design is to fix the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5).</p>
            <ul style="list-style-type: disc;">
                <li><b>Simple:</b> this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design in STARFlow</li>
                <li><b>End-to-end:</b> fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly.</li>
                <li><b>Good performance:</b> on the ImageNet 256x256 generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the prior work STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with REPA-E method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.</li>
            </ul>
            </p>    
            <d-figure>
                <figure>
                    <img src="assets/overview.png" alt="Method" style="width:100%; display: block; margin: auto;">
                    <figcaption>
                        <strong>Comparison of our framework with closely related methods.</strong>
                        Solid arrows indicate the forward pass,
                while dashed arrows denote gradient flows.
                We label frozen modules in
                <span style="background-color:#E8E8E8;">gray</span>,
                generative models in
                <span style="background-color:#D2F3D7;">green</span>,
                and VAE modules involved in training in
                <span style="background-color:#F9D1D4;">red</span>.
                    </figcaption>
                </figure>
            </d-figure>
        </section>

        <section id="insights">
            <h2>Key Insights</h2>
            <p><strong>1. VAEs with fixed variances are more robust to latent perturbation.</strong></p>
            <ul style="list-style-type: disc;">
                <li> A VAE with a large and fixed variance can maintain reconstruction quality under latent noise, while the performance of a VAE with learnable variance degrades significantly. 
                <li> For VAEs with a large variance, the images reconstructed from linearly interpolated latents still clearly show the main subjects (the cat or the dog), rather than blending them. 
            </ul>

            <d-figure>
                <figure>
                    <img src="assets/vae_robustness.png" alt="VAE Robustness" style="width:100%; display: block; margin: auto;">
                    <figcaption>
                        <strong>Robustness of VAEs with fixed variances.</strong>
                        (a) A VAE with a large and fixed variance can maintain reconstruction quality under latent noise, while the performance of a VAE with learnable variance degrades significantly. (b) For VAEs with a large variance, the images reconstructed from linearly interpolated latents still clearly show the main subjects (the cat or the dog), rather than blending them. 'Learnable' indicates a standard VAE with learnable variance, while '$\bar{\sigma}^2=x^2$' denotes a VAE with a fixed variance of $x^2$.
                    </figcaption>
                </figure>
            </d-figure>

            <p><strong>2. End-to-end training makes latent space possibly more suitable for developing generative models.</strong></p>
            <ul style="list-style-type: disc;">
                <li> Spectral entropy measures the randomness of frequency components; lower values indicate simpler data distributions in the frequency domain. 
                <li> End-to-end latent sapce has a lower ratio of high-frequency components. 
                <li> Total variation captures the overall local changes across tokens; lower values imply smoother latents. 
                <li> Autocorrelation reflects how similar a token sequence is to a shifted version of itself; higher autocorrelation indicates stronger spatial consistency.
            </ul>
            <d-figure>
                <figure>
                    <img src="assets/vae_latent_space.png" alt="VAE Latent Space" style="width:100%; display: block; margin: auto;">
                    <figcaption>
                        <strong>End-to-end training affects the latent space, making it possibly more suitable for developing generative models.</strong>
                    </figcaption>
                </figure>
            </d-figure>

            <p><strong>3. Variant studies.</strong></p>
            <ul style="list-style-type: disc;">
                <li> Only setting a fixed variance is not enough.</li>
            </ul>
            <ul style="list-style-type: disc;">
                <li> Good reconstruction does not mean good generation quality.</li>
            </ul>
            <ul style="list-style-type: disc;">
                <li> The end-to-end training significantly improves the generation quality.</li>

            <d-figure>
                <figure>
                    <img src="assets/variants.png" alt="Variant Studies" style="width:80%; display: block; margin: auto;">
                    <figcaption>
                        <strong>Variant studies.</strong>'Frozen VAE' means both VAE encoder and decoder are frozen during training. 'Frozen enc' means the decoder is trained. 'End-to-end' means VAE encoder and decoder, and the NF are jointly trained from scratch. 'Learnable var' means the variance is predicted by the VAE, while  'Fixed var' is our method with $\bar{\sigma}^2=0.5^2$. 'LN' denotes applying a layer normalization on the VAE encoder. 'Noise augmented' indicates adding Gaussian noise to VAE latents as done by STARFlow.
                    </figcaption>
                </figure>
            </d-figure>
            </div>
        </section>

        <section id="results">
            <h2>Experimental Results</h2>
            <p>
                On ImageNet class-conditional generation, our method SimFlow establishes a new state-of-the-art, significantly outperforming NF-based baselines.
            </p>

            <h3>ImageNet $256\times 256$</h3>
            <d-figure>
                <figure>
                    <img src="assets/results.png" alt="Results 256x256" style="width:100%; display: block; margin: auto;">
                    <figcaption><strong>Comparison on ImageNet $256\times 256$.</strong> SimFlow (with or without REPA-E) achieves better performance than STARFlow on ImageNet $256\times 256$.</figcaption>
                </figure>
            </d-figure>

            <h3>ImageNet $512\times 512$</h3>
            <d-figure>
                <figure>
                    <img src="assets/results_512.png" alt="Results 512x512" style="width:90%; display: block; margin: auto;">
                    <figcaption><strong>Comparison on ImageNet $512\times 512$.</strong> SimFlow with REPA-E achieves better performance than STARFlow on ImageNet $512\times 512$.</figcaption>
                </figure>
            </d-figure>

            <h3>Qualitative Results</h3>
            <d-figure>
                <figure>
                    <img src="assets/examples.png" alt="Examples" style="width:100%; display: block; margin: auto;">
                    <figcaption><strong>Selected samples.</strong> More uncurated samples can be found in the paper.</figcaption>
                </figure>
            </d-figure>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This paper presents SimFlow, an end-to-end training framework for latent NFs by simply fixing the VAE variance. This makes latent space smoother and helps NFs generalize better when sampling, without needing extra noise schedules or denoising steps. Experiments show that SimFlow improves generation quality and speeds up training compared to existing NF methods. Future work will expand this framework to text-to-image training and explore a second-stage training with the VAE fixed after joint training.
            </p>
            <div class="button-container" style="justify-content: center; margin-top: 1rem;">
                <a href="#" class="button">Read the Full Paper</a>
            </div>
        </section>

        <d-appendix>
            <h3>Appendix</h3>

            <p style="font-size: 14px; color: #666; margin-top: 20px;">
        We thank the <a href="https://sihyun.me/REPA/" style="color: #1E3A8A;">REPA</a>, <a href="https://end2end-diffusion.github.io/" style="color: #1E3A8A;">REPA-E</a>, and <a href="https://data-free-flow-distill.github.io/" style="color: #1E3A8A;">FreeFlow</a> projects for the website template.

            
            <p class="bibtex">
                @article{zhao2025simflow,<br>
                &nbsp;&nbsp;title={SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows},<br>
                &nbsp;&nbsp;author={Zhao, Qinyu and Zheng, Guangting and Yang, Tao and Zhu, Rui <br>
                &nbsp;&nbsp;&nbsp;&nbsp;and Leng, Xingjian and Gould, Stephen and Zheng, Liang},<br>
                &nbsp;&nbsp;year={2025},<br>
                &nbsp;&nbsp;journal={arXiv preprint},<br>
                }
            </p>
        </d-appendix>

        <d-bibliography src="bibliography.bib"></d-bibliography>

        
    </p>
    </d-article>


    <script src="contents_bar.js"></script>
</body>
</html>
